{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../performer_pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import argparse\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import random\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "from performer_pytorch import PerformerLM\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 3\n",
    "GRADIENT_ACCUMULATION = 60\n",
    "LEARNING_RATE = 1e-4\n",
    "SEQ_LEN = 16907\n",
    "VALIDATE_EVERY = 1\n",
    "CLASS = 7\n",
    "MASK_PROB = 0.15\n",
    "REPLACE_PROB = 0.9\n",
    "RANDOM_TOKEN_PROB = 0.\n",
    "MASK_TOKEN_ID = CLASS - 1\n",
    "PAD_TOKEN_ID = CLASS - 1\n",
    "MASK_IGNORE_TOKEN_IDS = [0]\n",
    "POS_EMBED_USING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'panglao_pretrain_1'\n",
    "ckpt_dir = './checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " local_rank = 1\n",
    "# rank = int(os.environ[\"RANK\"])\n",
    "# is_master = rank == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist.init_process_group(backend='nccl')\n",
    "# torch.cuda.set_device(local_rank)\n",
    "device = torch.device(\"cuda\", local_rank)\n",
    "# world_size = torch.distributed.get_world_size()\n",
    "torch.manual_seed(SEED)\n",
    "# seed_all(SEED + torch.distributed.get_rank())\n",
    "\n",
    "# get the random prob matrix and True means smaller than prob threshold\n",
    "def prob_mask_like(t, prob):\n",
    "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "# get the mask matrix which cannot be masked\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "\n",
    "def get_mask_subset_with_prob(mask, prob):\n",
    "    batch, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * seq_len)      # num of mask of a single sequence in average\n",
    "    num_tokens = mask.sum(dim=-1, keepdim=True)     # num of pure tokens of each sequence except special tokens\n",
    "    mask_excess = torch.cat((torch.zeros(0), torch.arange(mask.size(-1)).repeat(mask.size(0)))).reshape(mask.size(0),mask.size(-1)).to(device)\n",
    "    mask_excess = (mask_excess >= (num_tokens * prob).ceil())        # only 15% of pure tokens can be masked\n",
    "    mask_excess = mask_excess[:, :max_masked]       # get difference between 15% of pure tokens and 15% of all tokens\n",
    "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)     # rand (0-1) as prob, special token use -1e9\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)      # get index of topk prob to mask\n",
    "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)        # delete difference of mask not pure\n",
    "    new_mask = torch.zeros((batch, seq_len + 1), device=device)     # get (batch, seq_len) shape zero matrix\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)       # set masks in zero matrix as 1\n",
    "    return new_mask[:, 1:].bool()       # the final mask, True is mask\n",
    "\n",
    "def data_mask(data,\n",
    "    mask_prob = MASK_PROB,\n",
    "    replace_prob = REPLACE_PROB,\n",
    "    num_tokens = None,\n",
    "    random_token_prob = RANDOM_TOKEN_PROB,\n",
    "    mask_token_id = MASK_TOKEN_ID,\n",
    "    pad_token_id = PAD_TOKEN_ID,\n",
    "    mask_ignore_token_ids = MASK_IGNORE_TOKEN_IDS\n",
    "):\n",
    "    mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "    # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "    # also do not include these special tokens in the tokens chosen at random\n",
    "    no_mask = mask_with_tokens(data, mask_ignore_token_ids)   # ignore_token as True, will not be masked later\n",
    "    mask = get_mask_subset_with_prob(~no_mask, mask_prob)      # get the True/False mask matrix\n",
    "    # get mask indices\n",
    "    ## mask_indices = torch.nonzero(mask, as_tuple=True)   # get the index of mask(nonzero value of mask matrix)\n",
    "    # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "    masked_input = data.clone().detach()\n",
    "    # if random token probability > 0 for mlm\n",
    "    if random_token_prob > 0:\n",
    "        assert num_tokens is not None, 'num_tokens keyword must be supplied when instantiating MLM if using random token replacement'\n",
    "        random_token_prob = prob_mask_like(data, random_token_prob)       # get the mask matrix of random token replace\n",
    "        random_tokens = torch.randint(0, num_tokens, data.shape, device=data.device)     # generate random token matrix with the same shape as in\n",
    "        random_no_mask = mask_with_tokens(random_tokens, mask_ignore_token_ids)        # not masked matrix for the random token matrix\n",
    "        random_token_prob &= ~random_no_mask        # get the pure mask matrix of random token replace\n",
    "        random_indices = torch.nonzero(random_token_prob, as_tuple=True)        # index of random token replace\n",
    "        masked_input[random_indices] = random_tokens[random_indices]        # replace some tokens by random token\n",
    "    # [mask] input\n",
    "    replace_prob = prob_mask_like(data, replace_prob)     # get the mask matrix of token being masked\n",
    "    masked_input = masked_input.masked_fill(mask * replace_prob, mask_token_id)        # get the data has been masked by mask_token\n",
    "    # mask out any tokens to padding tokens that were not originally going to be masked\n",
    "    labels = data.masked_fill(~mask, pad_token_id)        # the label of masked tokens\n",
    "    return masked_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = random.randint(0, self.data.shape[0]-1)\n",
    "        full_seq = self.data[rand_start].toarray()[0]\n",
    "        full_seq[full_seq > (CLASS - 2)] = CLASS - 2\n",
    "        full_seq = torch.from_numpy(full_seq).long()\n",
    "        full_seq = torch.cat((full_seq, torch.tensor([0]))).to(device)\n",
    "        return full_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishwa/Enter/lib/python3.9/site-packages/anndata/_core/anndata.py:1818: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data = sc.read_h5ad('../data/panglao_human.h5ad')\n",
    "data = data.X\n",
    "data_train, data_val = train_test_split(data, test_size=0.05,random_state=SEED)\n",
    "\n",
    "train_dataset = SCDataset(data_train)\n",
    "val_dataset = SCDataset(data_val)\n",
    "\n",
    "# train_sampler = DistributedSampler(train_dataset)\n",
    "# val_sampler = SequentialDistributedSampler(val_dataset, batch_size=BATCH_SIZE, world_size=world_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE) #, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE) #, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PerformerLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPerformerLM\u001b[49m(\n\u001b[1;32m      2\u001b[0m     num_tokens \u001b[38;5;241m=\u001b[39m CLASS,\n\u001b[1;32m      3\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m      4\u001b[0m     depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m      5\u001b[0m     max_seq_len \u001b[38;5;241m=\u001b[39m SEQ_LEN,\n\u001b[1;32m      6\u001b[0m     heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      7\u001b[0m     local_attn_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      8\u001b[0m     g2v_position_emb \u001b[38;5;241m=\u001b[39m POS_EMBED_USING\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# model = DDP(model, device_ids=[local_rank], output_device=local_rank)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# optimizer\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PerformerLM' is not defined"
     ]
    }
   ],
   "source": [
    "model = PerformerLM(\n",
    "    num_tokens = CLASS,\n",
    "    dim = 200,\n",
    "    depth = 6,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    heads = 10,\n",
    "    local_attn_heads = 0,\n",
    "    g2v_position_emb = POS_EMBED_USING\n",
    ")\n",
    "model.to(device)\n",
    "# model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n",
    "\n",
    "# optimizer\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# learning rate scheduler\n",
    "scheduler = CosineAnnealingWarmupRestarts(\n",
    "    optimizer,\n",
    "    first_cycle_steps=15,\n",
    "    cycle_mult=2,\n",
    "    max_lr=LEARNING_RATE,\n",
    "    min_lr=1e-6,\n",
    "    warmup_steps=5,\n",
    "    gamma=0.9\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN_ID, reduction='mean').to(local_rank)\n",
    "softmax = nn.Softmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdist\u001b[49m\u001b[38;5;241m.\u001b[39mbarrier()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      3\u001b[0m     train_loader\u001b[38;5;241m.\u001b[39msampler\u001b[38;5;241m.\u001b[39mset_epoch(i)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dist' is not defined"
     ]
    }
   ],
   "source": [
    "dist.barrier()\n",
    "for i in range(1, EPOCHS+1):\n",
    "    train_loader.sampler.set_epoch(i)\n",
    "    model.train()\n",
    "    dist.barrier()\n",
    "    running_loss = 0.0\n",
    "    cum_acc = 0.0\n",
    "    for index, data in enumerate(train_loader):\n",
    "        index += 1\n",
    "        data = data.to(device)\n",
    "        data, labels = data_mask(data)\n",
    "        if index % GRADIENT_ACCUMULATION != 0:\n",
    "            with model.no_sync():\n",
    "                logits = model(data)\n",
    "                loss = loss_fn(logits.transpose(1, 2), labels) / GRADIENT_ACCUMULATION\n",
    "                loss.backward()\n",
    "        if index % GRADIENT_ACCUMULATION == 0:\n",
    "            logits = model(data)\n",
    "            loss = loss_fn(logits.transpose(1, 2), labels) / GRADIENT_ACCUMULATION\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), int(1e2))\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        running_loss += loss.item()\n",
    "        final = softmax(logits)[..., 1:-1]\n",
    "        final = final.argmax(dim=-1) + 1\n",
    "        pred_num = (labels != PAD_TOKEN_ID).sum(dim=-1)\n",
    "        correct_num = ((labels != PAD_TOKEN_ID) * (final == labels)).sum(dim=-1)\n",
    "        cum_acc += torch.true_divide(correct_num, pred_num).mean().item()\n",
    "    epoch_loss = running_loss / index\n",
    "    epoch_acc = 100 * cum_acc / index\n",
    "    epoch_loss = get_reduced(epoch_loss, local_rank, 0, world_size)\n",
    "    epoch_acc = get_reduced(epoch_acc, local_rank, 0, world_size)\n",
    "    if is_master:\n",
    "        print(f'    ==  Epoch: {i} | Training Loss: {epoch_loss:.6f} | Accuracy: {epoch_acc:6.4f}%  ==')\n",
    "    dist.barrier()\n",
    "    scheduler.step()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        dist.barrier()\n",
    "        running_loss = 0.0\n",
    "        running_error = 0.0\n",
    "        predictions = []\n",
    "        truths = []\n",
    "        with torch.no_grad():\n",
    "            for index, data in enumerate(val_loader):\n",
    "                index += 1\n",
    "                data = data.to(device)\n",
    "                data, labels = data_mask(data)\n",
    "                logits = model(data)\n",
    "                loss = loss_fn(logits.transpose(1, 2), labels)\n",
    "                running_loss += loss.item()\n",
    "                softmax = nn.Softmax(dim=-1)\n",
    "                final = softmax(logits)[..., 1:-1]\n",
    "                final = final.argmax(dim=-1) + 1\n",
    "                predictions.append(final)\n",
    "                truths.append(labels)\n",
    "            del data, labels, logits, final\n",
    "            # gather\n",
    "            predictions = distributed_concat(torch.cat(predictions, dim=0), len(val_sampler.dataset), world_size)\n",
    "            truths = distributed_concat(torch.cat(truths, dim=0), len(val_sampler.dataset), world_size)\n",
    "            correct_num = ((truths != PAD_TOKEN_ID) * (predictions == truths)).sum(dim=-1)[0].item()\n",
    "            val_num = (truths != PAD_TOKEN_ID).sum(dim=-1)[0].item()\n",
    "            val_loss = running_loss / index\n",
    "            val_loss = get_reduced(val_loss, local_rank, 0, world_size)\n",
    "        if is_master:\n",
    "            val_acc = 100 * correct_num / val_num\n",
    "            print(f'    ==  Epoch: {i} | Validation Loss: {val_loss:.6f} | Accuracy: {val_acc:6.4f}%  ==')\n",
    "    del predictions, truths\n",
    "\n",
    "    if is_master:\n",
    "        save_ckpt(i, model, optimizer, scheduler, epoch_loss, model_name, ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
