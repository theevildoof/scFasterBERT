{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyJkiTPbRpb-"
      },
      "source": [
        "# This is an upgraded version of the scBERT\n",
        "\n",
        "## We are planning to incorporate these things:\n",
        "\n",
        "### Improvements to the Encoder block\n",
        "1. Grouped Multi Query Attention\n",
        "2. RMS Norm in place of LayerNorm for faster training\n",
        "3. Flash attention 2.0\n",
        "4. SwiGLU/SiLU in place of ReLU/GLU - done\n",
        "5. Gene coexpression\n",
        "\n",
        "### Improvements to improve parameter count while reducing computational cost\n",
        "1. Mixtral of Experts\n",
        "\n",
        "### Improvements to training stratergy\n",
        "2. Improved Token Embeddings\n",
        "1. Improved masking\n",
        "\n",
        "### For Faster training\n",
        "1. Mixed precision training - done\n",
        "2. Distributed Data Parallel Training - done\n",
        "3. Faster Data Loading using MultDL\n",
        "4. Adafactor - https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#optimizer-choice\n",
        "5. Torch compile - https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#using-torchcompile\n",
        "6. Data preloading - done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OJGxG3-Ru5x"
      },
      "source": [
        "# Installing Necessary libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC94UqOfRxiZ"
      },
      "source": [
        "## Installing Flash Attention 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bfl0AqwJRuFS",
        "outputId": "2452f3cb-32c5-4f08-d227-eea120be8649"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y ninja && pip install ninja\n",
        "!pip install packaging\n",
        "!pip install flash-attn --no-build-isolation\n",
        "# !MAX_JOBS=4 pip install flash-attn --no-build-isolation if machine has less than 96 GB of RAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shj3kQ1G1ZnU"
      },
      "source": [
        "## HF Accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sah4E7k1cJR",
        "outputId": "3a85597a-ad54-42e8-de91-87653b101c95"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1xrNYlASp0W"
      },
      "source": [
        "# Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scbFsov3SV-9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAWSEfpM7XJs"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "from accelerate import notebook_launcher\n",
        "accelerator = Accelerator(mixed_precision='fp16', gradient_accumulation_steps=60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJoTJiCGc0yD"
      },
      "source": [
        "# Model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNVIueoglbJF"
      },
      "source": [
        "## RMS Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVi4FUa9lbdQ"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rmsnorm = RMSNorm(dim=1)\n",
        "x = torch.tensor([[10,0,3]])\n",
        "print(x.shape)\n",
        "rmsnorm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IIYcq-WjG33"
      },
      "source": [
        "## FeedForward Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ_9U1eHjKEu"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, args_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.w1 = nn.Linear(args_dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, args_dim, bias=False)\n",
        "        self.w3 = nn.Linear(args_dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, x) -> torch.Tensor:\n",
        "        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da9pIIOwktBT"
      },
      "outputs": [],
      "source": [
        "# test_ff = FeedForwardBlock(2,1)\n",
        "# test_ff(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23NjJeQCirLR"
      },
      "source": [
        "## Sparse Mixture of Experts block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evfn2B2WiogT"
      },
      "outputs": [],
      "source": [
        "class MoeLayer(nn.Module):\n",
        "    def __init__(self, experts: List[nn.Module], gate: nn.Module, num_experts_per_tok):\n",
        "        super().__init__()\n",
        "        assert len(experts) > 0\n",
        "        self.experts = nn.ModuleList(experts)\n",
        "        self.gate = gate\n",
        "        self.num_experts_per_tok = num_experts_per_tok\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor):\n",
        "        # For each token, generate `num_experts` logits indicating which expert to use.\n",
        "        gate_logits = self.gate(inputs)\n",
        "\n",
        "        # For each token, select the top `num_experts_per_tok` experts, and use them to compute\n",
        "        weights, selected_experts = torch.topk(gate_logits, self.args.num_experts_per_tok)\n",
        "\n",
        "        # Apply the softmax to the logits AFTER selecting the top-k, this makes comparison with different hyperparams consitent.\n",
        "        # Because even if we change the total number of experts or the number of experts per token, the sum of the weights will still be 1 for each token.\n",
        "        weights = F.softmax(weights, dim=1, dtype=torch.float).to(inputs.dtype)\n",
        "\n",
        "        results = torch.zeros_like(inputs)\n",
        "        for current_expert_index, current_expert in enumerate(self.experts):\n",
        "            # For each expert, select which token it will be applied to.\n",
        "            token_index, token_expert_index = torch.where(selected_experts == current_expert_index)\n",
        "            # Apply the expert to the selected tokens weighting it by the logits (post-softmax) computed above.\n",
        "            results[token_index] += weights[token_index, token_expert_index, None] * current_expert(\n",
        "                inputs[token_index]\n",
        "            )\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9asMrrwlio2c"
      },
      "source": [
        "## Attention Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Smc8RPEJc4CX"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embed_size, heads):\n",
        "    super().__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.head_dim = embed_size // heads\n",
        "\n",
        "    assert (self.head_dim * heads == embed_size), \"Embed size needs to be divided by heads\"\n",
        "\n",
        "    self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.queries = nn.Linear(self.head_dim, self. head_dim, bias=False)\n",
        "    self.fc_out = nn.Linear(heads*)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcEhqJCklIq3"
      },
      "source": [
        "## Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RamgJ8TwlIcc"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, embed_size, heads):\n",
        "    super().__init__()\n",
        "    self.attention = SelfAttention()\n",
        "    self.attention_norm = RMSNorm()\n",
        "    self.ff_norm = RMSNorm()\n",
        "    self.feed_forward = MoeLayer(experts=[FeedForward(args=args) for _ in range(args.moe.num_experts)], gate=nn.Linear(args.dim, args.moe.num_experts=8, bias=False), moe_args=args.moe)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    r = self.attention(self.attention_norm(x))\n",
        "    h = x + r\n",
        "    x = self.feed_forward(self.ffn_norm(h))\n",
        "    out = h + r\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v-3YzBlrBcT"
      },
      "source": [
        "## SCBERT 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMibJ236rDR1"
      },
      "outputs": [],
      "source": [
        "class scBERT2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__():\n",
        "\n",
        "  def forward(self, x):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKnCuV14te12"
      },
      "source": [
        "# Masking Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p8e7xtHtkOn"
      },
      "outputs": [],
      "source": [
        "# get the random prob matrix and True means smaller than prob threshold\n",
        "def prob_mask_like(t, prob):\n",
        "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
        "\n",
        "# get the mask matrix which cannot be masked\n",
        "def mask_with_tokens(t, token_ids):\n",
        "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
        "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
        "    return mask\n",
        "\n",
        "def get_mask_subset_with_prob(mask, prob):\n",
        "    batch, seq_len, device = *mask.shape, mask.device\n",
        "    max_masked = math.ceil(prob * seq_len)      # num of mask of a single sequence in average\n",
        "    num_tokens = mask.sum(dim=-1, keepdim=True)     # num of pure tokens of each sequence except special tokens\n",
        "    mask_excess = torch.cat((torch.zeros(0), torch.arange(mask.size(-1)).repeat(mask.size(0)))).reshape(mask.size(0),mask.size(-1)).to(device)\n",
        "    mask_excess = (mask_excess >= (num_tokens * prob).ceil())        # only 15% of pure tokens can be masked\n",
        "    mask_excess = mask_excess[:, :max_masked]       # get difference between 15% of pure tokens and 15% of all tokens\n",
        "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)     # rand (0-1) as prob, special token use -1e9\n",
        "    _, sampled_indices = rand.topk(max_masked, dim=-1)      # get index of topk prob to mask\n",
        "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)        # delete difference of mask not pure\n",
        "    new_mask = torch.zeros((batch, seq_len + 1), device=device)     # get (batch, seq_len) shape zero matrix\n",
        "    new_mask.scatter_(-1, sampled_indices, 1)       # set masks in zero matrix as 1\n",
        "    return new_mask[:, 1:].bool()       # the final mask, True is mask\n",
        "\n",
        "def data_mask(data,\n",
        "    mask_prob = MASK_PROB,\n",
        "    replace_prob = REPLACE_PROB,\n",
        "    num_tokens = None,\n",
        "    random_token_prob = RANDOM_TOKEN_PROB,\n",
        "    mask_token_id = MASK_TOKEN_ID,\n",
        "    pad_token_id = PAD_TOKEN_ID,\n",
        "    mask_ignore_token_ids = MASK_IGNORE_TOKEN_IDS\n",
        "):\n",
        "    mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
        "    # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
        "    # also do not include these special tokens in the tokens chosen at random\n",
        "    no_mask = mask_with_tokens(data, mask_ignore_token_ids)   # ignore_token as True, will not be masked later\n",
        "    mask = get_mask_subset_with_prob(~no_mask, mask_prob)      # get the True/False mask matrix\n",
        "    # get mask indices\n",
        "    ## mask_indices = torch.nonzero(mask, as_tuple=True)   # get the index of mask(nonzero value of mask matrix)\n",
        "    # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
        "    masked_input = data.clone().detach()\n",
        "    # if random token probability > 0 for mlm\n",
        "    if random_token_prob > 0:\n",
        "        assert num_tokens is not None, 'num_tokens keyword must be supplied when instantiating MLM if using random token replacement'\n",
        "        random_token_prob = prob_mask_like(data, random_token_prob)       # get the mask matrix of random token replace\n",
        "        random_tokens = torch.randint(0, num_tokens, data.shape, device=data.device)     # generate random token matrix with the same shape as in\n",
        "        random_no_mask = mask_with_tokens(random_tokens, mask_ignore_token_ids)        # not masked matrix for the random token matrix\n",
        "        random_token_prob &= ~random_no_mask        # get the pure mask matrix of random token replace\n",
        "        random_indices = torch.nonzero(random_token_prob, as_tuple=True)        # index of random token replace\n",
        "        masked_input[random_indices] = random_tokens[random_indices]        # replace some tokens by random token\n",
        "    # [mask] input\n",
        "    replace_prob = prob_mask_like(data, replace_prob)     # get the mask matrix of token being masked\n",
        "    masked_input = masked_input.masked_fill(mask * replace_prob, mask_token_id)        # get the data has been masked by mask_token\n",
        "    # mask out any tokens to padding tokens that were not originally going to be masked\n",
        "    labels = data.masked_fill(~mask, pad_token_id)        # the label of masked tokens\n",
        "    return masked_input, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2HCGtoPaUT3"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qZA5H3maXnK"
      },
      "source": [
        "## Token Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udu0INhMaWMk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "curHFKcGcjTR"
      },
      "source": [
        "## \"Positional Embedding\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeiYB1t-cyup"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unEiU-MgsRiU"
      },
      "source": [
        "# Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1n2sX8zscew"
      },
      "source": [
        "## Low RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm53EMYxsrGP"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_path = '../data/panglao_human.h5ad'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuACwldUsXY2"
      },
      "outputs": [],
      "source": [
        "class SCDataset(Dataset):\n",
        "    def __init__(self, file_path, indices):\n",
        "        self.file_path = file_path\n",
        "        self.data = sc.read_h5ad(data_path, backed='r')\n",
        "        self.length = self.data.X.shape[0]\n",
        "        self.indices = indices\n",
        "        self.indices_len = len(self.indices)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        rand_start = random.randint(0, self.indices_len-1)\n",
        "        data = self.data.X[self.indices[rand_start]]\n",
        "        # Convert sparse matrix row to dense if necessary\n",
        "        if isinstance(data, scipy.sparse.csr_matrix):\n",
        "            data = data.toarray().squeeze(0)\n",
        "            # print(data)\n",
        "\n",
        "        # Apply the same preprocessing as before\n",
        "        data[data > (CLASS - 2)] = CLASS - 2\n",
        "        data = torch.from_numpy(data).long()\n",
        "        data = torch.cat((data, torch.tensor([0]))).to(device)\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrFVZKwxsssA"
      },
      "outputs": [],
      "source": [
        "total_samples = 1357593  # Replace with the actual total length of your dataset\n",
        "train_ratio = 0.95\n",
        "\n",
        "# Calculate the number of samples in each set\n",
        "num_train_samples = int(total_samples * train_ratio)\n",
        "num_valid_samples = total_samples - num_train_samples\n",
        "\n",
        "# Generate indices for training and validation sets\n",
        "train_indices = list(range(0, num_train_samples))\n",
        "valid_indices = list(range(num_train_samples, total_samples))\n",
        "\n",
        "print(\"Training indices:\", len(train_indices))\n",
        "print(\"Validation indices:\", len(valid_indices))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4YISsvdsmdF"
      },
      "outputs": [],
      "source": [
        "train_dataset = SCDataset(data_path, train_indices)\n",
        "val_dataset = SCDataset(data_path, valid_indices)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdWJlwUXsgtV"
      },
      "source": [
        "## High RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7_ybNtls5LW"
      },
      "outputs": [],
      "source": [
        "class SCDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        rand_start = random.randint(0, self.data.shape[0]-1)\n",
        "        full_seq = self.data[rand_start].toarray()[0]\n",
        "        full_seq[full_seq > (CLASS - 2)] = CLASS - 2\n",
        "        full_seq = torch.from_numpy(full_seq).long()\n",
        "        full_seq = torch.cat((full_seq, torch.tensor([0]))).to(device)\n",
        "        return full_seq\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "data = sc.read_h5ad('/content/drive/MyDrive/scFasterBERT/data/panglao_human.h5ad')\n",
        "data = data.X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaLPSiXvs6dK"
      },
      "outputs": [],
      "source": [
        "data_train, data_val = train_test_split(data, test_size=0.05,random_state=SEED)\n",
        "\n",
        "train_dataset = SCDataset(data_train)\n",
        "val_dataset = SCDataset(data_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LclZhMJ4s8JM"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Ovs7VDrsm0"
      },
      "source": [
        "# Model Initialization and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1dN5oT2tN9X"
      },
      "outputs": [],
      "source": [
        "model = scBERT2().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um--zgZsrr9s"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(student_model.parameters(), lr=LEARNING_RATE)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN_ID, reduction='mean').to(device)\n",
        "softmax = nn.Softmax(dim=-1)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMQrge9b71FU"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, model, optimizer = accelerator.prepare(train_loader, val_loader, model, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "oXkno4fQtNQo",
        "outputId": "b74d6d29-fb22-4f80-f6e4-c87894ca4654"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 1\n",
        "VALIDATE_EVERY = 2\n",
        "GRADIENT_ACCUMULATION = 4\n",
        "PAD_TOKEN_ID = 0  # Assuming a placeholder value\n",
        "\n",
        "for i in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    cum_acc = 0.0\n",
        "    for index, data in tqdm(enumerate(train_loader)):\n",
        "        with accelerator.accumulate(model):\n",
        "          index += 1\n",
        "          data = data.to(device)\n",
        "          data, labels = data_mask(data)\n",
        "          logits = model(data)\n",
        "          loss = loss_fn(logits.transpose(1, 2), labels)\n",
        "          accelerator.backward(loss)\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), int(1e2))\n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "          optimizer.zero_grad()\n",
        "        running_loss += loss.item()\n",
        "        final = softmax(logits)[..., 1:-1]\n",
        "        final = final.argmax(dim=-1) + 1\n",
        "        pred_num = (labels != PAD_TOKEN_ID).sum(dim=-1)\n",
        "        correct_num = ((labels != PAD_TOKEN_ID) * (final == labels)).sum(dim=-1)\n",
        "        cum_acc += torch.true_divide(correct_num, pred_num).mean().item()\n",
        "\n",
        "    epoch_loss = running_loss / index\n",
        "    epoch_acc = 100 * cum_acc / index\n",
        "    print(f'    ==  Epoch: {i} | Training Loss: {epoch_loss:.6f} | Accuracy: {epoch_acc:6.4f}%  ==')\n",
        "\n",
        "    if i % VALIDATE_EVERY == 0:\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        predictions = []\n",
        "        truths = []\n",
        "        with torch.no_grad():\n",
        "            for index, data in tqdm(enumerate(val_loader)):\n",
        "                index += 1\n",
        "                data = data.to(device)\n",
        "                data, labels = data_mask(data)\n",
        "                logits = model(data)\n",
        "                loss = loss_fn(logits.transpose(1, 2), labels)\n",
        "                running_loss += loss.item()\n",
        "                softmax = nn.Softmax(dim=-1)\n",
        "                final = softmax(logits)[..., 1:-1]\n",
        "                final = final.argmax(dim=-1) + 1\n",
        "                predictions.append(final)\n",
        "                truths.append(labels)\n",
        "        val_loss = running_loss / index\n",
        "        correct_num = ((torch.cat(truths, dim=0) != PAD_TOKEN_ID) * (torch.cat(predictions, dim=0) == torch.cat(truths, dim=0))).sum().item()\n",
        "        val_num = (torch.cat(truths, dim=0) != PAD_TOKEN_ID).sum().item()\n",
        "        val_acc = 100 * correct_num / val_num\n",
        "        print(f'    ==  Epoch: {i} | Validation Loss: {val_loss:.6f} | Accuracy: {val_acc:6.4f}%  ==')\n",
        "\n",
        "    # save_ckpt(i, model, optimizerepoch_loss, model_name, ckpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2s9avFEAJyT"
      },
      "outputs": [],
      "source": [
        "notebook_launcher(train_model, args=(), num_processes=2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
