{"cells":[{"cell_type":"markdown","metadata":{"id":"cyJkiTPbRpb-"},"source":["# This is an upgraded version of the scBERT\n","\n","## We are planning to incorporate these things:\n","\n","### Improvements to the Encoder block\n","1. Grouped Multi Query Attention\n","2. RMS Norm in place of LayerNorm for faster training\n","3. Flash attention 2.0\n","4. SwiGLU/SiLU in place of ReLU/GLU - done\n","5. Gene coexpression\n","\n","### Improvements to improve parameter count while reducing computational cost\n","1. Mixtral of Experts\n","\n","### Improvements to training stratergy\n","2. Improved Token Embeddings\n","1. Improved masking\n","\n","### For Faster training\n","1. Mixed precision training - done\n","2. Distributed Data Parallel Training - done\n","3. Faster Data Loading using MultDL\n","4. Adafactor - https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#optimizer-choice\n","5. Torch compile - https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#using-torchcompile\n","6. Data preloading - done"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32816,"status":"ok","timestamp":1715743524095,"user":{"displayName":"Vishwa","userId":"12540788039158208250"},"user_tz":-330},"id":"rfWT4jWk5PLz","outputId":"4e03a813-5cfe-4860-d2ad-68c0137b83f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71327,"status":"ok","timestamp":1715743595418,"user":{"displayName":"Vishwa","userId":"12540788039158208250"},"user_tz":-330},"id":"4ozGzNvj39E5","outputId":"768a49a9-b192-4c4d-a72a-da436a26397e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mixture_of_experts\n","  Downloading mixture_of_experts-0.2.3-py3-none-any.whl (6.0 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mixture_of_experts) (2.2.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mixture_of_experts) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mixture_of_experts) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mixture_of_experts) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mixture_of_experts) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mixture_of_experts) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mixture_of_experts) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->mixture_of_experts)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->mixture_of_experts)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->mixture_of_experts)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->mixture_of_experts)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->mixture_of_experts)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->mixture_of_experts)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->mixture_of_experts)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->mixture_of_experts)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->mixture_of_experts)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch->mixture_of_experts)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->mixture_of_experts)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->mixture_of_experts) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->mixture_of_experts)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mixture_of_experts) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mixture_of_experts) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, mixture_of_experts\n","Successfully installed mixture_of_experts-0.2.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n","Collecting scanpy\n","  Downloading scanpy-1.10.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting anndata>=0.8 (from scanpy)\n","  Downloading anndata-0.10.7-py3-none-any.whl (122 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.9.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4.2)\n","Collecting legacy-api-wrap>=1.4 (from scanpy)\n","  Downloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n","Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.7.1)\n","Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.4.0)\n","Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.3)\n","Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.58.1)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.25.2)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (24.0)\n","Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from scanpy) (2.0.3)\n","Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.6)\n","Collecting pynndescent>=0.5 (from scanpy)\n","  Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.2.2)\n","Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.11.4)\n","Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.1)\n","Collecting session-info (from scanpy)\n","  Downloading session_info-1.0.0.tar.gz (24 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.66.4)\n","Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n","  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting array-api-compat!=1.5,>1.4 (from anndata>=0.8->scanpy)\n","  Downloading array_api_compat-1.6-py3-none-any.whl (36 kB)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.2.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.41.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->scanpy) (3.5.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy->scanpy) (1.16.0)\n","Collecting stdlib_list (from session-info->scanpy)\n","  Downloading stdlib_list-0.10.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: session-info\n","  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8026 sha256=aaefc3ef8f4b7f086e62a610424ffa84848683f48aa4ea346ce206b9bd7f0102\n","  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n","Successfully built session-info\n","Installing collected packages: array-api-compat, stdlib_list, legacy-api-wrap, session-info, pynndescent, anndata, umap-learn, scanpy\n","Successfully installed anndata-0.10.7 array-api-compat-1.6 legacy-api-wrap-1.4 pynndescent-0.5.12 scanpy-1.10.1 session-info-1.0.0 stdlib_list-0.10.0 umap-learn-0.5.6\n","Collecting accelerate\n","  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.30.1\n"]}],"source":["%pip install mixture_of_experts\n","%pip install scanpy\n","%pip install accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwLyKQLr4J_0"},"outputs":[],"source":["import scanpy as sc\n","from sklearn.model_selection import train_test_split\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from mixture_of_experts import MoE\n","from torchsummary import summary\n","from torch.utils.data import Dataset, DataLoader\n","import random\n","from tqdm import tqdm\n","from functools import reduce\n","from torch.optim import AdamW\n","import math\n","import matplotlib.pyplot as plt\n","from accelerate import Accelerator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pH3xJ3i3G08h"},"outputs":[],"source":["accelerator = Accelerator(mixed_precision='fp16')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RefixN0a4MzJ"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_oE9DHqe4PqM"},"outputs":[],"source":["torch.backends.cuda.enable_flash_sdp(True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBkF9In54QqA"},"outputs":[],"source":["class FlashAttentionBlock(nn.Module):\n","    def __init__(self, d_model: int, h: int) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.h = h\n","        assert d_model % h ==0, \"d_model is not divisble by h\"\n","        self.d_k = d_model // h\n","\n","        self.w_q = nn.Linear(d_model, d_model)\n","        self.w_k = nn.Linear(d_model, d_model)\n","        self.w_v = nn.Linear(d_model, d_model)\n","        self.w_o = nn.Linear(d_model, d_model)\n","\n","\n","    def forward(self, q, k, v):\n","        query = self.w_q (q) # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n","        key = self.w_k(k) # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n","        value = self.w_v(v) # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n","\n","        # Test code\n","        # query = q\n","        # key = k\n","        # value = v\n","\n","        # (batch, seq_len, d_model) -> (Batch, seq_len, h, d_k) -> (Batch, h, seq_len, d_k)\n","        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n","        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n","        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n","\n","        x = F.scaled_dot_product_attention(query,key,value, dropout_p=0.1)\n","\n","        # (Batch, h, seq_len, d_k) -> (Batch, seq_len, h, d_k) -> (Batch, seq_len, d_model)\n","        x =  x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n","\n","        # (Batch, seq_len, d_model)  -> (Batch, seq_len, d_model)\n","        return self.w_o(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D0m_TQXc4RuW"},"outputs":[],"source":["class RMSNorm(torch.nn.Module):\n","    def __init__(self, dim: int, eps: float = 1e-6):\n","        super().__init__()\n","        self.eps = eps\n","        self.weight = nn.Parameter(torch.ones(dim))\n","\n","    def _norm(self, x):\n","        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n","\n","    def forward(self, x):\n","        output = self._norm(x.float()).type_as(x)\n","        return output * self.weight"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YP35shDE4Txm"},"outputs":[],"source":["moe = MoE(\n","    dim = 200,\n","    num_experts =  4,               # increase the experts (# parameters) of your model without increasing computation\n","    hidden_dim = 200 * 4,           # size of hidden dimension in each expert, defaults to 4 * dimension\n","    activation = nn.SiLU,      # use your preferred activation, will default to GELU\n","    second_policy_train = 'random', # in top_2 gating, policy for whether to use a second-place expert\n","    second_policy_eval = 'random',  # all (always) | none (never) | threshold (if gate value > the given threshold) | random (if gate value > threshold * random_uniform(0, 1))\n","    second_threshold_train = 0.2,\n","    second_threshold_eval = 0.2,\n","    capacity_factor_train = 1.25,   # experts have fixed capacity per batch. we need some extra capacity in case gating is not perfectly balanced.\n","    capacity_factor_eval = 2.,      # capacity_factor_* should be set to a value >=1\n","    loss_coef = 1e-2                # multiplier on the auxiliary expert balancing auxiliary loss\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRekhvRm4WMi"},"outputs":[],"source":["class Encoder(nn.Module):\n","  def __init__(self, local_heads, d_model, hidden_ff_model):\n","    super().__init__()\n","    # Embedding dimension = 200, Local_Attention heads = 10\n","    self.attention = FlashAttentionBlock(d_model= d_model, h=local_heads)\n","    self.attention_norm = RMSNorm(dim =d_model)\n","    self.ff_norm = RMSNorm(dim=d_model)\n","    self.feed_forward = MoE(dim =d_model, num_experts=8, hidden_dim= hidden_ff_model,activation = nn.SiLU, second_policy_train = 'random', second_policy_eval = 'random', second_threshold_train = 0.2, second_threshold_eval = 0.2, capacity_factor_train = 1.25,capacity_factor_eval = 2., loss_coef = 1e-2)\n","\n","\n","  def forward(self, x):\n","    x_normed = self.attention_norm(x)\n","    # print(x_normed.shape)\n","    r = self.attention(x_normed, x_normed, x_normed)\n","    h = x + r\n","    r, _ = self.feed_forward(self.ff_norm(h))\n","    out = h + r\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eujUIx5t4X5Z"},"outputs":[],"source":["class Gene2VecPositionalEmbedding(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        gene2vec_weight = np.load('/content/drive/MyDrive/scFasterBERT/data/gene2vec_16906.npy')\n","        gene2vec_weight = np.concatenate((gene2vec_weight, np.zeros((1, gene2vec_weight.shape[1]))), axis=0)\n","        gene2vec_weight = torch.from_numpy(gene2vec_weight)\n","        self.emb = nn.Embedding.from_pretrained(gene2vec_weight)\n","\n","    def forward(self, x):\n","        t = torch.arange(x.shape[1], device = device)\n","        return self.emb(t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPqtFB0w4ZJN"},"outputs":[],"source":["#max_seq_len =16907\n","class scBERT2(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.token_emb = nn.Embedding(7, 200)\n","        self.pos_emb = Gene2VecPositionalEmbedding()\n","        self.layer1 = Encoder(local_heads=10,d_model=200, hidden_ff_model=400)\n","        self.layer2 = Encoder(local_heads=10,d_model=200, hidden_ff_model=400)\n","        self.layer3 = Encoder(local_heads=10,d_model=200, hidden_ff_model=400)\n","        self.layer4 = Encoder(local_heads=10,d_model=200, hidden_ff_model=400)\n","        self.layer5 = Encoder(local_heads=10,d_model=200, hidden_ff_model=400)\n","        self.layer6 = Encoder(local_heads=10,d_model=200, hidden_ff_model=400)\n","        self.norm = RMSNorm(200)\n","        self.classifier = nn.Linear(in_features=200, out_features=7)\n","\n","    def forward(self, x):\n","    # x = x.type(torch.int32)\n","        pos_emb = self.pos_emb(x)\n","        x = self.token_emb(x.int())\n","        # print(x.shape)\n","        x += pos_emb\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.layer5(x)\n","        x = self.layer6(x)\n","        x = self.classifier(self.norm(x))\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-SYLX5b4a76"},"outputs":[],"source":["scbert = scBERT2()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1715743607415,"user":{"displayName":"Vishwa","userId":"12540788039158208250"},"user_tz":-330},"id":"L_Dy8JMF4bfU","outputId":"4a94af63-3014-4037-9d6f-e99cd25d8270"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["8659807"]},"metadata":{},"execution_count":14}],"source":["sum(p.numel() for p in scbert.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2J1JEZx4kyE"},"outputs":[],"source":["state_dict = scbert.state_dict()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UbCyEmf4lF-"},"outputs":[],"source":["ckpt = torch.load('/content/drive/MyDrive/scFasterBERT/panglao_pretrain.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOsUmSID4nkE"},"outputs":[],"source":["state_dict['layer1.attention.w_q.weight']=ckpt['model_state_dict']['performer.net.layers.0.0.fn.to_q.weight']\n","state_dict['layer1.attention.w_k.weight']=ckpt['model_state_dict']['performer.net.layers.0.0.fn.to_k.weight']\n","state_dict['layer1.attention.w_v.weight']=ckpt['model_state_dict']['performer.net.layers.0.0.fn.to_v.weight']\n","state_dict['layer1.attention.w_o.weight']=ckpt['model_state_dict']['performer.net.layers.0.0.fn.to_out.weight']\n","state_dict['layer1.attention.w_o.bias']=ckpt['model_state_dict']['performer.net.layers.0.0.fn.to_out.bias']\n","\n","state_dict['layer2.attention.w_q.weight']=ckpt['model_state_dict']['performer.net.layers.1.0.fn.to_q.weight']\n","state_dict['layer2.attention.w_k.weight']=ckpt['model_state_dict']['performer.net.layers.1.0.fn.to_k.weight']\n","state_dict['layer2.attention.w_v.weight']=ckpt['model_state_dict']['performer.net.layers.1.0.fn.to_v.weight']\n","state_dict['layer2.attention.w_o.weight']=ckpt['model_state_dict']['performer.net.layers.1.0.fn.to_out.weight']\n","state_dict['layer2.attention.w_o.bias']=ckpt['model_state_dict']['performer.net.layers.1.0.fn.to_out.bias']\n","\n","state_dict['layer3.attention.w_q.weight']=ckpt['model_state_dict']['performer.net.layers.2.0.fn.to_q.weight']\n","state_dict['layer3.attention.w_k.weight']=ckpt['model_state_dict']['performer.net.layers.2.0.fn.to_k.weight']\n","state_dict['layer3.attention.w_v.weight']=ckpt['model_state_dict']['performer.net.layers.2.0.fn.to_v.weight']\n","state_dict['layer3.attention.w_o.weight']=ckpt['model_state_dict']['performer.net.layers.2.0.fn.to_out.weight']\n","state_dict['layer3.attention.w_o.bias']=ckpt['model_state_dict']['performer.net.layers.2.0.fn.to_out.bias']\n","\n","state_dict['layer4.attention.w_q.weight']=ckpt['model_state_dict']['performer.net.layers.3.0.fn.to_q.weight']\n","state_dict['layer4.attention.w_k.weight']=ckpt['model_state_dict']['performer.net.layers.3.0.fn.to_k.weight']\n","state_dict['layer4.attention.w_v.weight']=ckpt['model_state_dict']['performer.net.layers.3.0.fn.to_v.weight']\n","state_dict['layer4.attention.w_o.weight']=ckpt['model_state_dict']['performer.net.layers.3.0.fn.to_out.weight']\n","state_dict['layer4.attention.w_o.bias']=ckpt['model_state_dict']['performer.net.layers.3.0.fn.to_out.bias']\n","\n","state_dict['layer5.attention.w_q.weight']=ckpt['model_state_dict']['performer.net.layers.4.0.fn.to_q.weight']\n","state_dict['layer5.attention.w_k.weight']=ckpt['model_state_dict']['performer.net.layers.4.0.fn.to_k.weight']\n","state_dict['layer5.attention.w_v.weight']=ckpt['model_state_dict']['performer.net.layers.4.0.fn.to_v.weight']\n","state_dict['layer5.attention.w_o.weight']=ckpt['model_state_dict']['performer.net.layers.4.0.fn.to_out.weight']\n","state_dict['layer5.attention.w_o.bias']=ckpt['model_state_dict']['performer.net.layers.4.0.fn.to_out.bias']\n","\n","state_dict['layer6.attention.w_q.weight']=ckpt['model_state_dict']['performer.net.layers.5.0.fn.to_q.weight']\n","state_dict['layer6.attention.w_k.weight']=ckpt['model_state_dict']['performer.net.layers.5.0.fn.to_k.weight']\n","state_dict['layer6.attention.w_v.weight']=ckpt['model_state_dict']['performer.net.layers.5.0.fn.to_v.weight']\n","state_dict['layer6.attention.w_o.weight']=ckpt['model_state_dict']['performer.net.layers.5.0.fn.to_out.weight']\n","state_dict['layer6.attention.w_o.bias']=ckpt['model_state_dict']['performer.net.layers.5.0.fn.to_out.bias']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXFNl4W25s-f"},"outputs":[],"source":["# scbert.load_state_dict(state_dict)"]},{"cell_type":"markdown","metadata":{"id":"3TtEj_Pp4prU"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJoNKp634oGg"},"outputs":[],"source":["SEED = 2021\n","EPOCHS = 10\n","BATCH_SIZE = 1\n","LEARNING_RATE = 1e-4\n","GRADIENT_ACCUMULATION = 60\n","SEQ_LEN = 16907\n","VALIDATE_EVERY = 1\n","CLASS = 7\n","MASK_PROB = 0.15\n","REPLACE_PROB = 0.9\n","RANDOM_TOKEN_PROB = 0.\n","MASK_TOKEN_ID = CLASS - 1\n","PAD_TOKEN_ID = CLASS - 1\n","MASK_IGNORE_TOKEN_IDS = [0]\n","POS_EMBED_USING = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fim-97q04q2K"},"outputs":[],"source":["# get the random prob matrix and True means smaller than prob threshold\n","def prob_mask_like(t, prob):\n","    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n","\n","# get the mask matrix which cannot be masked\n","def mask_with_tokens(t, token_ids):\n","    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n","    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n","    return mask\n","\n","def get_mask_subset_with_prob(mask, prob):\n","    batch, seq_len, device = *mask.shape, mask.device\n","    max_masked = math.ceil(prob * seq_len)      # num of mask of a single sequence in average\n","    num_tokens = mask.sum(dim=-1, keepdim=True)     # num of pure tokens of each sequence except special tokens\n","    mask_excess = torch.cat((torch.zeros(0), torch.arange(mask.size(-1)).repeat(mask.size(0)))).reshape(mask.size(0),mask.size(-1)).to(device)\n","    mask_excess = (mask_excess >= (num_tokens * prob).ceil())        # only 15% of pure tokens can be masked\n","    mask_excess = mask_excess[:, :max_masked]       # get difference between 15% of pure tokens and 15% of all tokens\n","    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)     # rand (0-1) as prob, special token use -1e9\n","    _, sampled_indices = rand.topk(max_masked, dim=-1)      # get index of topk prob to mask\n","    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)        # delete difference of mask not pure\n","    new_mask = torch.zeros((batch, seq_len + 1), device=device)     # get (batch, seq_len) shape zero matrix\n","    new_mask.scatter_(-1, sampled_indices, 1)       # set masks in zero matrix as 1\n","    return new_mask[:, 1:].bool()       # the final mask, True is mask\n","\n","def data_mask(data,\n","    mask_prob = MASK_PROB,\n","    replace_prob = REPLACE_PROB,\n","    num_tokens = None,\n","    random_token_prob = RANDOM_TOKEN_PROB,\n","    mask_token_id = MASK_TOKEN_ID,\n","    pad_token_id = PAD_TOKEN_ID,\n","    mask_ignore_token_ids = MASK_IGNORE_TOKEN_IDS\n","):\n","    mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n","    # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n","    # also do not include these special tokens in the tokens chosen at random\n","    no_mask = mask_with_tokens(data, mask_ignore_token_ids)   # ignore_token as True, will not be masked later\n","    mask = get_mask_subset_with_prob(~no_mask, mask_prob)      # get the True/False mask matrix\n","    # get mask indices\n","    ## mask_indices = torch.nonzero(mask, as_tuple=True)   # get the index of mask(nonzero value of mask matrix)\n","    # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n","    masked_input = data.clone().detach()\n","    # if random token probability > 0 for mlm\n","    if random_token_prob > 0:\n","        assert num_tokens is not None, 'num_tokens keyword must be supplied when instantiating MLM if using random token replacement'\n","        random_token_prob = prob_mask_like(data, random_token_prob)       # get the mask matrix of random token replace\n","        random_tokens = torch.randint(0, num_tokens, data.shape, device=data.device)     # generate random token matrix with the same shape as in\n","        random_no_mask = mask_with_tokens(random_tokens, mask_ignore_token_ids)        # not masked matrix for the random token matrix\n","        random_token_prob &= ~random_no_mask        # get the pure mask matrix of random token replace\n","        random_indices = torch.nonzero(random_token_prob, as_tuple=True)        # index of random token replace\n","        masked_input[random_indices] = random_tokens[random_indices]        # replace some tokens by random token\n","    # [mask] input\n","    replace_prob = prob_mask_like(data, replace_prob)     # get the mask matrix of token being masked\n","    masked_input = masked_input.masked_fill(mask * replace_prob, mask_token_id)        # get the data has been masked by mask_token\n","    # mask out any tokens to padding tokens that were not originally going to be masked\n","    labels = data.masked_fill(~mask, pad_token_id)        # the label of masked tokens\n","    return masked_input, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hK_Vi9i04tqG"},"outputs":[],"source":["class SCDataset(Dataset):\n","    def __init__(self, data):\n","        super().__init__()\n","        self.data = data\n","\n","    def __getitem__(self, index):\n","        rand_start = random.randint(0, self.data.shape[0]-1)\n","        full_seq = self.data[rand_start].toarray()[0]\n","        full_seq[full_seq > (CLASS - 2)] = CLASS - 2\n","        full_seq = torch.from_numpy(full_seq).long()\n","        full_seq = torch.cat((full_seq, torch.tensor([0]))).to(device)\n","        return full_seq\n","\n","    def __len__(self):\n","        return self.data.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2294,"status":"ok","timestamp":1715743622413,"user":{"displayName":"Vishwa","userId":"12540788039158208250"},"user_tz":-330},"id":"XatEzl_p4uUu","outputId":"d945654d-cc6e-4de6-9985-784ad12e9c92"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/anndata/_core/anndata.py:1818: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n","  utils.warn_names_duplicates(\"obs\")\n"]},{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{},"execution_count":22}],"source":["data = sc.read_h5ad('/content/drive/MyDrive/scFasterBERT/data/panglao_human_small.h5ad')\n","data = data.X\n","data_train, data_val = train_test_split(data, test_size=0.1,random_state=SEED)\n","\n","train_dataset = SCDataset(data_train)\n","val_dataset = SCDataset(data_val)\n","len(val_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1715743622413,"user":{"displayName":"Vishwa","userId":"12540788039158208250"},"user_tz":-330},"id":"AtZkF6Vq4xiR","outputId":"30296fc5-a945-4469-aa67-c0a5ade884ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 9000)"]},"metadata":{},"execution_count":23}],"source":["train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","len(val_loader), len(train_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xUZ65Pho4yol"},"outputs":[],"source":["optimizer = AdamW(scbert.parameters(), lr=LEARNING_RATE)\n","loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN_ID, reduction='mean').to(device)\n","softmax = nn.Softmax(dim=-1)\n","scaler = torch.cuda.amp.GradScaler()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XaSpql-YG8Fc"},"outputs":[],"source":["train_loader, val_loader, scbert, optimizer = accelerator.prepare(train_loader, val_loader, scbert, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBXucvQjF6gD"},"outputs":[],"source":["def train2(model):\n","  softmax = nn.Softmax(dim=-1)\n","  train_losses =[]\n","  train_accuracies = []\n","  valid_losses =[]\n","  valid_accuracies = []\n","  for i in range(1, EPOCHS+1):\n","      model.train()\n","      running_loss = 0.0\n","      cum_acc = 0.0\n","      for index, data in enumerate(tqdm(train_loader)):\n","          index += 1\n","          data = data.to(device)\n","          data, labels = data_mask(data)\n","          logits = model(data)\n","          loss = loss_fn(logits.transpose(1, 2), labels)/ GRADIENT_ACCUMULATION\n","          accelerator.backward(loss)\n","          if index % GRADIENT_ACCUMULATION == 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), int(1e2))\n","            optimizer.step()\n","            optimizer.zero_grad()\n","          running_loss += loss.item()\n","          final = softmax(logits)[..., 1:-1]\n","          final = final.argmax(dim=-1) + 1\n","          pred_num = (labels != PAD_TOKEN_ID).sum(dim=-1)\n","          correct_num = ((labels != PAD_TOKEN_ID) * (final == labels)).sum(dim=-1)\n","          cum_acc += torch.true_divide(correct_num, pred_num).mean().item()\n","      epoch_loss = running_loss / index\n","      epoch_acc = 100 * cum_acc / index\n","      train_losses.append(epoch_loss)\n","      train_accuracies.append(epoch_acc)\n","      print(f'    ==  Epoch: {i} | Training Loss: {epoch_loss:.6f} | Accuracy: {epoch_acc:6.4f}%  ==')\n","\n","      if i % VALIDATE_EVERY == 0:\n","          model.eval()\n","          running_loss = 0.0\n","          predictions = []\n","          truths = []\n","          with torch.no_grad():\n","              for index, data in enumerate(tqdm(val_loader)):\n","                  index += 1\n","                  data = data.to(device)\n","                  data, labels = data_mask(data)\n","                  logits = model(data)\n","                  loss = loss_fn(logits.transpose(1, 2), labels)\n","                  running_loss += loss.item()\n","                  softmax = nn.Softmax(dim=-1)\n","                  final = softmax(logits)[..., 1:-1]\n","                  final = final.argmax(dim=-1) + 1\n","                  predictions.append(final)\n","                  truths.append(labels)\n","          val_loss = running_loss / index\n","          correct_num = ((torch.cat(truths, dim=0) != PAD_TOKEN_ID) * (torch.cat(predictions, dim=0) == torch.cat(truths, dim=0))).sum().item()\n","          val_num = (torch.cat(truths, dim=0) != PAD_TOKEN_ID).sum().item()\n","          val_acc = 100 * correct_num / val_num\n","          valid_losses.append(val_loss)\n","          valid_accuracies.append(val_acc)\n","          print(f'    ==  Epoch: {i} | Validation Loss: {val_loss:.6f} | Accuracy: {val_acc:6.4f}%  ==')\n","      torch.save(model.state_dict(), f'scbert2_epoch{i}.pth')\n","\n","  return train_losses, train_accuracies, valid_losses, valid_accuracies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRoz2AWiHf4e","colab":{"base_uri":"https://localhost:8080/","height":373},"outputId":"b891992e-570e-4770-89e9-6baa2ce6211d","executionInfo":{"status":"error","timestamp":1715748323821,"user_tz":-330,"elapsed":4701413,"user":{"displayName":"Vishwa","userId":"12540788039158208250"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9000/9000 [1:12:33<00:00,  2.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["    ==  Epoch: 1 | Training Loss: 0.013173 | Accuracy: 70.3014%  ==\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [05:11<00:00,  3.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["    ==  Epoch: 1 | Validation Loss: 0.526640 | Accuracy: 76.8060%  ==\n"]},{"output_type":"stream","name":"stderr","text":["  1%|          | 71/9000 [00:34<1:12:09,  2.06it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-afafe73d51a0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-26-51ed1c2809e4>\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m           \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mGRADIENT_ACCUMULATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-8adeae0dbeb8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-f77d5adeaa3e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_normed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_normed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_normed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mixture_of_experts/mixture_of_experts.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_experts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mdispatch_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombine_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mexpert_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bnd,bnec->ebcd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispatch_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mixture_of_experts/mixture_of_experts.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, importance)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;34m*\u001b[0m \u001b[0mmask_1_flat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_gates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;34m*\u001b[0m \u001b[0msafe_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_in_expert_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_capacity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0mgate_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;34m*\u001b[0m \u001b[0mmask_2_flat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mixture_of_experts/mixture_of_experts.py\u001b[0m in \u001b[0;36msafe_one_hot\u001b[0;34m(indexes, max_length)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msafe_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mmax_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minit_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train_losses, train_accuracies, valid_losses, valid_accuracies = train2(scbert)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R09VwupeCeut"},"outputs":[],"source":["def plot_graphs(train_losses, train_accuracies, valid_losses, valid_accuracies):\n","  plt.plot(train_losses)\n","  plt.plot(train_accuracies)\n","  plt.plot(valid_losses)\n","  plt.plot(valid_accuracies)\n","  plt.legend(['train_loss', 'train_accuracy', 'valid_loss', 'valid_accuracy'])\n","  plt.title('Panglao_human')\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gG9AmRcDWyt","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"error","timestamp":1715748331936,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vishwa","userId":"12540788039158208250"}},"outputId":"9d471b0b-9dcb-4655-ffe8-f3ca928243bb"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'function' object has no attribute 'train_losses'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-b9b1aefabcef>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'train_losses'"]}],"source":["plot_graphs(train2.train_losses, train2.train_accuracies, train2.valid_losses, train2.valid_accuracies)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}