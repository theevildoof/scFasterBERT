{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: grouped-query-attention-pytorch in /home/vishwa/Enter/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: einops~=0.6.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from grouped-query-attention-pytorch) (0.6.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from grouped-query-attention-pytorch) (2.2.1)\n",
      "Requirement already satisfied: torchscale~=0.2.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from grouped-query-attention-pytorch) (0.2.0)\n",
      "Requirement already satisfied: jinja2 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (4.10.0)\n",
      "Requirement already satisfied: fsspec in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (12.1.105)\n",
      "Requirement already satisfied: filelock in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (3.13.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (8.9.2.26)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (12.1.3.1)\n",
      "Requirement already satisfied: networkx in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (11.4.5.107)\n",
      "Requirement already satisfied: sympy in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (1.12)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch>=1.8.0->grouped-query-attention-pytorch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/vishwa/Enter/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->grouped-query-attention-pytorch) (12.4.99)\n",
      "Requirement already satisfied: fairscale==0.4.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from torchscale~=0.2.0->grouped-query-attention-pytorch) (0.4.0)\n",
      "Requirement already satisfied: timm==0.4.12 in /home/vishwa/Enter/lib/python3.9/site-packages (from torchscale~=0.2.0->grouped-query-attention-pytorch) (0.4.12)\n",
      "Requirement already satisfied: torchvision in /home/vishwa/Enter/lib/python3.9/site-packages (from timm==0.4.12->torchscale~=0.2.0->grouped-query-attention-pytorch) (0.17.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from jinja2->torch>=1.8.0->grouped-query-attention-pytorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/vishwa/Enter/lib/python3.9/site-packages (from sympy->torch>=1.8.0->grouped-query-attention-pytorch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from torchvision->timm==0.4.12->torchscale~=0.2.0->grouped-query-attention-pytorch) (10.2.0)\n",
      "Requirement already satisfied: numpy in /home/vishwa/Enter/lib/python3.9/site-packages (from torchvision->timm==0.4.12->torchscale~=0.2.0->grouped-query-attention-pytorch) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mixture_of_experts in /home/vishwa/Enter/lib/python3.9/site-packages (0.2.3)\n",
      "Requirement already satisfied: torch in /home/vishwa/Enter/lib/python3.9/site-packages (from mixture_of_experts) (2.2.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (11.0.2.54)\n",
      "Requirement already satisfied: fsspec in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (2.19.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (10.3.2.106)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (2.2.0)\n",
      "Requirement already satisfied: jinja2 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (12.1.105)\n",
      "Requirement already satisfied: networkx in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (3.2.1)\n",
      "Requirement already satisfied: filelock in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (1.12)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/vishwa/Enter/lib/python3.9/site-packages (from torch->mixture_of_experts) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/vishwa/Enter/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mixture_of_experts) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vishwa/Enter/lib/python3.9/site-packages (from jinja2->torch->mixture_of_experts) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/vishwa/Enter/lib/python3.9/site-packages (from sympy->torch->mixture_of_experts) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mixture_of_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mixture_of_experts import MoE\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "from torch.optim import Adam\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_flash_sdp(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Example Usage:\n",
    "# query, key, value = torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device), torch.randn(2, 3, 8, device=device)\n",
    "\n",
    "# with torch.backends.cuda.sdp_kernel(enable_flash=True):\n",
    "#     print(F.scaled_dot_product_attention(query,key,value).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model % h ==0, \"d_model is not divisble by h\"\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        query = self.w_q (q) # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        \n",
    "        # Test code\n",
    "        # query = q\n",
    "        # key = k\n",
    "        # value = v\n",
    "\n",
    "        # (batch, seq_len, d_model) -> (Batch, seq_len, h, d_k) -> (Batch, h, seq_len, d_k) \n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "\n",
    "        x = F.scaled_dot_product_attention(query,key,value, dropout_p=0.1)\n",
    "\n",
    "        # (Batch, h, seq_len, d_k) -> (Batch, seq_len, h, d_k) -> (Batch, seq_len, d_model) \n",
    "        x =  x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # (Batch, seq_len, d_model)  -> (Batch, seq_len, d_model) \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16906\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Enter/lib/python3.9/site-packages/torchsummary/torchsummary.py:68\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     65\u001b[0m hooks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# register hook\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m model(\u001b[38;5;241m*\u001b[39mx)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "# summary(model(query, key, value), (1, 16906, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor = torch.zeros(100, 200)\n",
    "\n",
    "# # Repeat this pattern across all columns\n",
    "# for i in range(0, 200, 20):\n",
    "#     tensor[:, i:i+20] = i\n",
    "\n",
    "# print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16906, 200])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = FlashAttentionBlock(200, 2).to(device)\n",
    "\n",
    "# query, key, value = torch.ones(1, 16906, 200, device=device), torch.ones(1, 16906, 200, device=device), torch.ones(1, 16906, 200, device=device)\n",
    "\n",
    "# model(query, key, value).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rmsnorm = RMSNorm(200)\n",
    "\n",
    "# x = torch.ones(1, 16906,200)\n",
    "\n",
    "# rmsnorm(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "moe = MoE(\n",
    "    dim = 200,\n",
    "    num_experts =  4,               # increase the experts (# parameters) of your model without increasing computation\n",
    "    hidden_dim = 200 * 4,           # size of hidden dimension in each expert, defaults to 4 * dimension\n",
    "    activation = nn.SiLU,      # use your preferred activation, will default to GELU\n",
    "    second_policy_train = 'random', # in top_2 gating, policy for whether to use a second-place expert\n",
    "    second_policy_eval = 'random',  # all (always) | none (never) | threshold (if gate value > the given threshold) | random (if gate value > threshold * random_uniform(0, 1))\n",
    "    second_threshold_train = 0.2,\n",
    "    second_threshold_eval = 0.2,\n",
    "    capacity_factor_train = 1.25,   # experts have fixed capacity per batch. we need some extra capacity in case gating is not perfectly balanced.\n",
    "    capacity_factor_eval = 2.,      # capacity_factor_* should be set to a value >=1\n",
    "    loss_coef = 1e-2                # multiplier on the auxiliary expert balancing auxiliary loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16906, 200])\n"
     ]
    }
   ],
   "source": [
    "# inputs = torch.ones(1, 16906, 200)\n",
    "# out, _ = moe(inputs) # (4, 1024, 512), (1,)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, local_heads, d_model, hidden_ff_model):\n",
    "    super().__init__()\n",
    "    # Embedding dimension = 200, Local_Attention heads = 10\n",
    "    self.attention = FlashAttentionBlock(d_model= d_model, h=local_heads)\n",
    "    self.attention_norm = RMSNorm(dim =d_model)\n",
    "    self.ff_norm = RMSNorm(dim=d_model)\n",
    "    self.feed_forward = MoE(dim =d_model, num_experts=8, hidden_dim= hidden_ff_model,activation = nn.SiLU, second_policy_train = 'random', second_policy_eval = 'random', second_threshold_train = 0.2, second_threshold_eval = 0.2, capacity_factor_train = 1.25,capacity_factor_eval = 2., loss_coef = 1e-2)                \n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x_normed = self.attention_norm(x)\n",
    "    # print(x_normed.shape)\n",
    "    r = self.attention(x_normed, x_normed, x_normed)\n",
    "    h = x + r\n",
    "    r, _ = self.feed_forward(self.ff_norm(h))\n",
    "    out = h + r\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(local_heads=10,d_model=200, hidden_ff_model=400).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(encoder.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16906, 200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16906, 200])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.ones(1, 16906, 200).to('cuda')\n",
    "encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1442800"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in encoder.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gene2VecPositionalEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        gene2vec_weight = np.load('../data/gene2vec_16906.npy')\n",
    "        gene2vec_weight = np.concatenate((gene2vec_weight, np.zeros((1, gene2vec_weight.shape[1]))), axis=0)\n",
    "        gene2vec_weight = torch.from_numpy(gene2vec_weight)\n",
    "        self.emb = nn.Embedding.from_pretrained(gene2vec_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device = device)\n",
    "        return self.emb(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_seq_len =16907\n",
    "class scBERT2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(7, 200)\n",
    "        self.pos_emb = Gene2VecPositionalEmbedding()\n",
    "        self.layer1 = Encoder(local_heads=10,d_model=200, hidden_ff_model=400)\n",
    "        self.layer2 = Encoder(local_heads=10,d_model=200, hidden_ff_model=400) \n",
    "        self.layer3 = Encoder(local_heads=10,d_model=200, hidden_ff_model=400)\n",
    "        self.layer4 = Encoder(local_heads=10,d_model=200, hidden_ff_model=400)\n",
    "        self.layer5 = Encoder(local_heads=10,d_model=200, hidden_ff_model=400)\n",
    "        self.layer6 = Encoder(local_heads=10,d_model=200, hidden_ff_model=400)\n",
    "        self.norm = RMSNorm(200)\n",
    "        self.classifier = nn.Linear(in_features=200, out_features=7)\n",
    "    \n",
    "    def forward(self, x):\n",
    "    # x = x.type(torch.int32)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "        x = self.token_emb(x.int())\n",
    "        # print(x.shape)\n",
    "        x += pos_emb\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.classifier(self.norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.ones(1, 16906).to('cuda')\n",
    "scbert = scBERT2().to('cuda')\n",
    "# scbert(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scBERT2(\n",
       "  (token_emb): Embedding(7, 200)\n",
       "  (pos_emb): Gene2VecPositionalEmbedding(\n",
       "    (emb): Embedding(16907, 200)\n",
       "  )\n",
       "  (layer1): Encoder(\n",
       "    (attention): FlashAttentionBlock(\n",
       "      (w_q): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_k): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_v): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_o): Linear(in_features=200, out_features=200, bias=True)\n",
       "    )\n",
       "    (attention_norm): RMSNorm()\n",
       "    (ff_norm): RMSNorm()\n",
       "    (feed_forward): MoE(\n",
       "      (gate): Top2Gating()\n",
       "      (experts): Experts(\n",
       "        (act): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Encoder(\n",
       "    (attention): FlashAttentionBlock(\n",
       "      (w_q): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_k): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_v): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_o): Linear(in_features=200, out_features=200, bias=True)\n",
       "    )\n",
       "    (attention_norm): RMSNorm()\n",
       "    (ff_norm): RMSNorm()\n",
       "    (feed_forward): MoE(\n",
       "      (gate): Top2Gating()\n",
       "      (experts): Experts(\n",
       "        (act): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Encoder(\n",
       "    (attention): FlashAttentionBlock(\n",
       "      (w_q): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_k): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_v): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_o): Linear(in_features=200, out_features=200, bias=True)\n",
       "    )\n",
       "    (attention_norm): RMSNorm()\n",
       "    (ff_norm): RMSNorm()\n",
       "    (feed_forward): MoE(\n",
       "      (gate): Top2Gating()\n",
       "      (experts): Experts(\n",
       "        (act): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Encoder(\n",
       "    (attention): FlashAttentionBlock(\n",
       "      (w_q): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_k): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_v): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_o): Linear(in_features=200, out_features=200, bias=True)\n",
       "    )\n",
       "    (attention_norm): RMSNorm()\n",
       "    (ff_norm): RMSNorm()\n",
       "    (feed_forward): MoE(\n",
       "      (gate): Top2Gating()\n",
       "      (experts): Experts(\n",
       "        (act): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer5): Encoder(\n",
       "    (attention): FlashAttentionBlock(\n",
       "      (w_q): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_k): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_v): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_o): Linear(in_features=200, out_features=200, bias=True)\n",
       "    )\n",
       "    (attention_norm): RMSNorm()\n",
       "    (ff_norm): RMSNorm()\n",
       "    (feed_forward): MoE(\n",
       "      (gate): Top2Gating()\n",
       "      (experts): Experts(\n",
       "        (act): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer6): Encoder(\n",
       "    (attention): FlashAttentionBlock(\n",
       "      (w_q): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_k): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_v): Linear(in_features=200, out_features=200, bias=True)\n",
       "      (w_o): Linear(in_features=200, out_features=200, bias=True)\n",
       "    )\n",
       "    (attention_norm): RMSNorm()\n",
       "    (ff_norm): RMSNorm()\n",
       "    (feed_forward): MoE(\n",
       "      (gate): Top2Gating()\n",
       "      (experts): Experts(\n",
       "        (act): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (classifier): Linear(in_features=200, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8659807"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in scbert.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = scbert.state_dict()\n",
    "# list(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('../panglao_pretrain.pth')\n",
    "# list(ckpt['model_state_dict'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_dict['layer1.attention.w_q.weight']=ckpt['model_state_dict']['performer.net.layers.0.0.fn.to_q.weight']\n",
    "state_dict['layer1.attention.w_k.weight']=ckpt['model_state_dict']['performer.net.layers.0.0.fn.to_k.weight']\n",
    "state_dict['layer1.attention.w_v.weight']=ckpt['model_state_dict']['performer.net.layers.0.0.fn.to_v.weight']\n",
    "state_dict['layer1.attention.w_o.weight']=ckpt['model_state_dict']['performer.net.layers.0.0.fn.to_out.weight']\n",
    "state_dict['layer1.attention.w_o.bias']=ckpt['model_state_dict']['performer.net.layers.0.0.fn.to_out.bias']\n",
    "\n",
    "state_dict['layer2.attention.w_q.weight']=ckpt['model_state_dict']['performer.net.layers.1.0.fn.to_q.weight']\n",
    "state_dict['layer2.attention.w_k.weight']=ckpt['model_state_dict']['performer.net.layers.1.0.fn.to_k.weight']\n",
    "state_dict['layer2.attention.w_v.weight']=ckpt['model_state_dict']['performer.net.layers.1.0.fn.to_v.weight']\n",
    "state_dict['layer2.attention.w_o.weight']=ckpt['model_state_dict']['performer.net.layers.1.0.fn.to_out.weight']\n",
    "state_dict['layer2.attention.w_o.bias']=ckpt['model_state_dict']['performer.net.layers.1.0.fn.to_out.bias']\n",
    "\n",
    "state_dict['layer3.attention.w_q.weight']=ckpt['model_state_dict']['performer.net.layers.2.0.fn.to_q.weight']\n",
    "state_dict['layer3.attention.w_k.weight']=ckpt['model_state_dict']['performer.net.layers.2.0.fn.to_k.weight']\n",
    "state_dict['layer3.attention.w_v.weight']=ckpt['model_state_dict']['performer.net.layers.2.0.fn.to_v.weight']\n",
    "state_dict['layer3.attention.w_o.weight']=ckpt['model_state_dict']['performer.net.layers.2.0.fn.to_out.weight']\n",
    "state_dict['layer3.attention.w_o.bias']=ckpt['model_state_dict']['performer.net.layers.2.0.fn.to_out.bias']\n",
    "\n",
    "state_dict['layer4.attention.w_q.weight']=ckpt['model_state_dict']['performer.net.layers.3.0.fn.to_q.weight']\n",
    "state_dict['layer4.attention.w_k.weight']=ckpt['model_state_dict']['performer.net.layers.3.0.fn.to_k.weight']\n",
    "state_dict['layer4.attention.w_v.weight']=ckpt['model_state_dict']['performer.net.layers.3.0.fn.to_v.weight']\n",
    "state_dict['layer4.attention.w_o.weight']=ckpt['model_state_dict']['performer.net.layers.3.0.fn.to_out.weight']\n",
    "state_dict['layer4.attention.w_o.bias']=ckpt['model_state_dict']['performer.net.layers.3.0.fn.to_out.bias']\n",
    "\n",
    "state_dict['layer5.attention.w_q.weight']=ckpt['model_state_dict']['performer.net.layers.4.0.fn.to_q.weight']\n",
    "state_dict['layer5.attention.w_k.weight']=ckpt['model_state_dict']['performer.net.layers.4.0.fn.to_k.weight']\n",
    "state_dict['layer5.attention.w_v.weight']=ckpt['model_state_dict']['performer.net.layers.4.0.fn.to_v.weight']\n",
    "state_dict['layer5.attention.w_o.weight']=ckpt['model_state_dict']['performer.net.layers.4.0.fn.to_out.weight']\n",
    "state_dict['layer5.attention.w_o.bias']=ckpt['model_state_dict']['performer.net.layers.4.0.fn.to_out.bias']\n",
    "\n",
    "state_dict['layer6.attention.w_q.weight']=ckpt['model_state_dict']['performer.net.layers.5.0.fn.to_q.weight']\n",
    "state_dict['layer6.attention.w_k.weight']=ckpt['model_state_dict']['performer.net.layers.5.0.fn.to_k.weight']\n",
    "state_dict['layer6.attention.w_v.weight']=ckpt['model_state_dict']['performer.net.layers.5.0.fn.to_v.weight']\n",
    "state_dict['layer6.attention.w_o.weight']=ckpt['model_state_dict']['performer.net.layers.5.0.fn.to_out.weight']\n",
    "state_dict['layer6.attention.w_o.bias']=ckpt['model_state_dict']['performer.net.layers.5.0.fn.to_out.bias']\n",
    "\n",
    "state_dict['to_out.weight']=ckpt['model_state_dict']['to_out.weight']\n",
    "state_dict['to_out.bias']=ckpt['model_state_dict']['to_out.bias']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION = 60\n",
    "LEARNING_RATE = 1e-4\n",
    "SEQ_LEN = 16907\n",
    "VALIDATE_EVERY = 1\n",
    "CLASS = 7\n",
    "MASK_PROB = 0.15\n",
    "REPLACE_PROB = 0.9\n",
    "RANDOM_TOKEN_PROB = 0.\n",
    "MASK_TOKEN_ID = CLASS - 1\n",
    "PAD_TOKEN_ID = CLASS - 1\n",
    "MASK_IGNORE_TOKEN_IDS = [0]\n",
    "POS_EMBED_USING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the random prob matrix and True means smaller than prob threshold\n",
    "def prob_mask_like(t, prob):\n",
    "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "# get the mask matrix which cannot be masked\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "\n",
    "def get_mask_subset_with_prob(mask, prob):\n",
    "    batch, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * seq_len)      # num of mask of a single sequence in average\n",
    "    num_tokens = mask.sum(dim=-1, keepdim=True)     # num of pure tokens of each sequence except special tokens\n",
    "    mask_excess = torch.cat((torch.zeros(0), torch.arange(mask.size(-1)).repeat(mask.size(0)))).reshape(mask.size(0),mask.size(-1)).to(device)\n",
    "    mask_excess = (mask_excess >= (num_tokens * prob).ceil())        # only 15% of pure tokens can be masked\n",
    "    mask_excess = mask_excess[:, :max_masked]       # get difference between 15% of pure tokens and 15% of all tokens\n",
    "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)     # rand (0-1) as prob, special token use -1e9\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)      # get index of topk prob to mask\n",
    "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)        # delete difference of mask not pure\n",
    "    new_mask = torch.zeros((batch, seq_len + 1), device=device)     # get (batch, seq_len) shape zero matrix\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)       # set masks in zero matrix as 1\n",
    "    return new_mask[:, 1:].bool()       # the final mask, True is mask\n",
    "\n",
    "def data_mask(data,\n",
    "    mask_prob = MASK_PROB,\n",
    "    replace_prob = REPLACE_PROB,\n",
    "    num_tokens = None,\n",
    "    random_token_prob = RANDOM_TOKEN_PROB,\n",
    "    mask_token_id = MASK_TOKEN_ID,\n",
    "    pad_token_id = PAD_TOKEN_ID,\n",
    "    mask_ignore_token_ids = MASK_IGNORE_TOKEN_IDS\n",
    "):\n",
    "    mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "    # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "    # also do not include these special tokens in the tokens chosen at random\n",
    "    no_mask = mask_with_tokens(data, mask_ignore_token_ids)   # ignore_token as True, will not be masked later\n",
    "    mask = get_mask_subset_with_prob(~no_mask, mask_prob)      # get the True/False mask matrix\n",
    "    # get mask indices\n",
    "    ## mask_indices = torch.nonzero(mask, as_tuple=True)   # get the index of mask(nonzero value of mask matrix)\n",
    "    # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "    masked_input = data.clone().detach()\n",
    "    # if random token probability > 0 for mlm\n",
    "    if random_token_prob > 0:\n",
    "        assert num_tokens is not None, 'num_tokens keyword must be supplied when instantiating MLM if using random token replacement'\n",
    "        random_token_prob = prob_mask_like(data, random_token_prob)       # get the mask matrix of random token replace\n",
    "        random_tokens = torch.randint(0, num_tokens, data.shape, device=data.device)     # generate random token matrix with the same shape as in\n",
    "        random_no_mask = mask_with_tokens(random_tokens, mask_ignore_token_ids)        # not masked matrix for the random token matrix\n",
    "        random_token_prob &= ~random_no_mask        # get the pure mask matrix of random token replace\n",
    "        random_indices = torch.nonzero(random_token_prob, as_tuple=True)        # index of random token replace\n",
    "        masked_input[random_indices] = random_tokens[random_indices]        # replace some tokens by random token\n",
    "    # [mask] input\n",
    "    replace_prob = prob_mask_like(data, replace_prob)     # get the mask matrix of token being masked\n",
    "    masked_input = masked_input.masked_fill(mask * replace_prob, mask_token_id)        # get the data has been masked by mask_token\n",
    "    # mask out any tokens to padding tokens that were not originally going to be masked\n",
    "    labels = data.masked_fill(~mask, pad_token_id)        # the label of masked tokens\n",
    "    return masked_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rand_start = random.randint(0, self.data.shape[0]-1)\n",
    "        full_seq = self.data[rand_start].toarray()[0]\n",
    "        full_seq[full_seq > (CLASS - 2)] = CLASS - 2\n",
    "        full_seq = torch.from_numpy(full_seq).long()\n",
    "        full_seq = torch.cat((full_seq, torch.tensor([0]))).to(device)\n",
    "        return full_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishwa/Enter/lib/python3.9/site-packages/anndata/_core/anndata.py:1818: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = sc.read_h5ad('panglao_human_small.h5ad')\n",
    "data = data.X\n",
    "data_train, data_val = train_test_split(data, test_size=0.1,random_state=SEED)\n",
    "\n",
    "train_dataset = SCDataset(data_train)\n",
    "val_dataset = SCDataset(data_val)\n",
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 9000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "len(val_loader), len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(scbert.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN_ID, reduction='mean').to(device)\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses =[]\n",
    "train_accuracies = []\n",
    "valid_losses =[]\n",
    "valid_accuracies = []\n",
    "\n",
    "def train(model):\n",
    "    train_losses =[]\n",
    "    train_accuracies = []\n",
    "    valid_losses =[]\n",
    "    valid_accuracies = []\n",
    "    for i in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        cum_acc = 0.0\n",
    "        for index, data in enumerate(tqdm(train_loader)):\n",
    "            # with accelerator.accumulate(model):\n",
    "            index += 1\n",
    "            data = data.to(device)\n",
    "            data, labels = data_mask(data)\n",
    "            logits = model(data)\n",
    "            loss = loss_fn(logits.transpose(1, 2), labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), int(1e2))\n",
    "            optimizer.step()\n",
    "        #   scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            final = softmax(logits)[..., 1:-1]\n",
    "            final = final.argmax(dim=-1) + 1\n",
    "            pred_num = (labels != PAD_TOKEN_ID).sum(dim=-1)\n",
    "            correct_num = ((labels != PAD_TOKEN_ID) * (final == labels)).sum(dim=-1)\n",
    "            cum_acc += torch.true_divide(correct_num, pred_num).mean().item()\n",
    "    \n",
    "        epoch_loss = running_loss / index\n",
    "        epoch_acc = 100 * cum_acc / index\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        print(f'    ==  Epoch: {i} | Training Loss: {epoch_loss:.6f} | Accuracy: {epoch_acc:6.4f}%  ==')\n",
    "    \n",
    "        if i % VALIDATE_EVERY == 0:\n",
    "            model.eval()\n",
    "            running_loss = 0.0\n",
    "            predictions = []\n",
    "            truths = []\n",
    "            with torch.no_grad():\n",
    "                for index, data in tqdm(enumerate(val_loader)):\n",
    "                    index += 1\n",
    "                    data = data.to(device)\n",
    "                    data, labels = data_mask(data)\n",
    "                    logits = model(data)\n",
    "                    loss = loss_fn(logits.transpose(1, 2), labels)\n",
    "                    running_loss += loss.item()\n",
    "                    softmax = nn.Softmax(dim=-1)\n",
    "                    final = softmax(logits)[..., 1:-1]\n",
    "                    final = final.argmax(dim=-1) + 1\n",
    "                    predictions.append(final)\n",
    "                    truths.append(labels)\n",
    "            val_loss = running_loss / index\n",
    "            correct_num = ((torch.cat(truths, dim=0) != PAD_TOKEN_ID) * (torch.cat(predictions, dim=0) == torch.cat(truths, dim=0))).sum().item()\n",
    "            val_num = (torch.cat(truths, dim=0) != PAD_TOKEN_ID).sum().item()\n",
    "            val_acc = 100 * correct_num / val_num\n",
    "            valid_losses.append(val_loss)\n",
    "            valid_accuracies.append(val_acc)\n",
    "            print(f'    ==  Epoch: {i} | Validation Loss: {val_loss:.6f} | Accuracy: {val_acc:6.4f}%  ==')\n",
    "    \n",
    "    return train_losses, train_accuracies, valid_losses, valid_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_losses, train_accuracies, valid_losses, valid_accuracies = train(scbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
